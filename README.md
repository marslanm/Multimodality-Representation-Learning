# Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications

## Multimodal Deep Learnig based Research

- [Survey Papers](#survey)
- [Task-specific Methods](#Task-specific-Methods)
- [Pretrainig Approaches](#Pretraining-Approaches)
- [Multimodal Applications](#Multimodal-Applications) ([Understanidng](#Understanding), [Classification](#Classification), [Generation](#Generation), [Retrieval](#Retrieval), [Translation](#Translation))
- [Multimodal Datasets](#Multimodal-Datasets)

# Survey

[**Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications.**](http://arxiv.org/abs/2302.00389)[1st Feb., 2023] <br>
*[Muhammad Arslan Manzoor](https://scholar.google.com/citations?hl=en&user=ZvXClnUAAAAJ), [Sarah Albarri](), [Ziting Xian](https://scholar.google.com/citations?hl=zh-CN&user=G7VId5YAAAAJ&view_op=list_works&gmla=AJsN-F6TTZmzbi9CmBIRLNRpAhcgmzH-nOUd8hM5UTjfT5A_mYW2ABzjSrdX7ki9GFgGaId2dLlMtXBkfq7X_qzYOwF_OuvCCthMiuVNUuUiac-aGoSwsKQ), [Zaiqiao Meng](https://scholar.google.com/citations?user=5jJKFVcAAAAJ&hl=en), [Preslav Nakov](https://scholar.google.com/citations?user=DfXsKZ4AAAAJ&hl=en), and [Shangsong Liang](https://scholar.google.com/citations?user=4uggVcIAAAAJ&h).*<br>
[[PDF](https://arxiv.org/abs/2302.00389)] 

**Vision-Language Pre-training:Basics, Recent Advances, and Future Trends.**[17th Oct, 2022]<br>
*Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao.*<br>
[[PDF](https://arxiv.org/pdf/2210.09263.pdf)]

**VLP: A survey on vision-language pre-training.**[18th Feb, 2022]<br>
*Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, and Bo Xu.*<br>
[[PDF](https://link.springer.com/article/10.1007/s11633-022-1369-5)]


**A Survey of Vision-Language Pre-Trained Models.**[18th Feb, 2022]<br>
*Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao.*<br>
[[PDF](https://arxiv.org/abs/2202.10936)]


**Vision-and-Language Pretrained Models: A Survey.**[15th Apr, 2022]<br>
*Siqu Long, Feiqi Cao, Soyeon Caren Han, Haiqin Yang.*<br>
[[PDF](https://arxiv.org/abs/2204.07356)]

**Comprehensive reading list for Multimodal Literature** <br>
[[Github](https://github.com/pliang279/awesome-multimodal-ml#survey-papers)]

**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.**[28th Jul, 2021]<br>
*Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig*<br>
[[PDF](https://arxiv.org/abs/2107.13586)]

**Recent Advances and Trends in Multimodal Deep Learning: A Review.**[24th May, 2021]<br>
*Jabeen Summaira, Xi Li, Amin Muhammad Shoib, Songyuan Li, Jabbar Abdul.*<br>
[[PDF](https://arxiv.org/pdf/2105.11087)]



# Task-specific-Methods

[Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network](https://ojs.aaai.org/index.php/AAAI/article/view/16258)

[Cascaded Recurrent Neural Networks for Hyperspectral Image Classification](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8662780)

[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html)

[Microsoft COCO: common objects in context](https://arxiv.org/pdf/1405.0312.pdf%090.949.pdf)

[Multimodal deep learning](http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf)

[Extracting and composing robust features with denoising autoencoders](https://dl.acm.org/doi/pdf/10.1145/1390156.1390294)

[Multi-gate attention network for image captioning](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9382255)

[AMC: attention guided multi-modal correlation learning for image search](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_AMC_Attention_guided_CVPR_2017_paper.pdf)

[Video captioning via hierarchical reinforcement learning](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf)

[Gaussian process with graph convolutional kernel for relational learning](https://dl.acm.org/doi/pdf/10.1145/3447548.3467327)

[Multi-relational graph representation learning with bayesian gaussian process network](https://doi.org/10.1609/aaai.v36i5.20492)

# Pretraining-Approaches

[Learning audio-visual speech representation by masked multimodal
cluster prediction](https://arxiv.org/abs/2201.02184)

[Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html)

[A survey of vision-language pre-trained models](https://arxiv.org/abs/2202.10936)

[Attention is all you need](https://proceedings.neurips.cc/paper/7181-attention-is-all)

[Vinvl: Revisiting visual representations in vision-language models](http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.html)

[M6: Multi-modality-to-multi-modality multitask mega-transformer for unified pretraining](https://dl.acm.org/doi/abs/10.1145/3447548.3467206)

[AMMU: a survey of transformer-based biomedical pretrained language models](https://www.sciencedirect.com/science/article/pii/S1532046421003117)

[Electra: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)

[Roberta: A robustly optimized bert pretraining approach](https://arxiv.org/abs/1907.11692)

[Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)

[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://academic.oup.com/bioinformatics/article-abstract/36/4/1234/5566506)

[Hatebert: Retraining bert for abusive language detection in english](https://arxiv.org/abs/2010.12472)

[InfoXLM: An information-theoretic framework for cross-lingual language model pre-training](https://arxiv.org/abs/2007.07834)

[Pre-training technique to localize medical bert and enhance biomedical bert](https://arxiv.org/abs/2005.07202)

[Don't stop pretraining: Adapt language models to domains and tasks](https://arxiv.org/abs/2004.10964)

[Knowledge inheritance for pre-trained language models](https://arxiv.org/abs/2105.13880)

[Improving language understanding by generative pre-training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

[Shuffled-token detection for refining pre-trained roberta](https://aclanthology.org/2021.naacl-srw.12/)

[Albert: A lite bert for self-supervised learning of language representations](https://arxiv.org/abs/1909.11942)

[Exploring the limits of transfer learning with a unified text-to-text transformer](https://dl.acm.org/doi/abs/10.5555/3455716.3455856)

[End-to-end object detection with transformers](https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13)

[Deformable detr: Deformable transformers for end-to-end object detection](https://arxiv.org/abs/2010.04159)

[Unified vision-language pre-training for image captioning and vqa](https://ojs.aaai.org/index.php/AAAI/article/view/7005)

[Virtex: Learning visual representations from textual annotations](http://openaccess.thecvf.com/content/CVPR2021/html/Desai_VirTex_Learning_Visual_Representations_From_Textual_Annotations_CVPR_2021_paper.html)

[Ernie-vil: Knowledge enhanced vision-language representations through scene graphs](https://ojs.aaai.org/index.php/AAAI/article/view/16431)

[Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://link.springer.com/chapter/10.1007/978-3-030-58577-8_8)

[Vokenization: Improving language understanding with contextualized, visual-grounded supervision](https://arxiv.org/abs/2010.06775)

[Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models](http://openaccess.thecvf.com/content_iccv_2015/html/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.html)

[Distributed representations of words and phrases and their compositionality](https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)

[Allennlp: A deep semantic natural language processing platform](https://arxiv.org/abs/1803.07640)

[Climbing towards NLU: On meaning, form, and understanding in the age of data](https://aclanthology.org/2020.acl-main.463/)

[Experience grounds language](https://arxiv.org/abs/2004.10151)

[HuBERT: How much can a bad teacher benefit ASR pre-training?](https://ieeexplore.ieee.org/abstract/document/9414460/)

## Unifying Achitectures

[Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)

[Improving language understanding by generative pre-training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

[End-to-end object detection with transformers](https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13)

[Uniter: Universal image-text representation learning](https://link.springer.com/chapter/10.1007/978-3-030-58577-8_7)

[Unit: Multimodal multitask learning with a unified transformer](https://openaccess.thecvf.com/content/ICCV2021/html/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com)

[Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text](https://proceedings.neurips.cc/paper/2021/hash/cb3213ada48302953cb0f166464ab356-Abstract.html)

[Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework](https://proceedings.mlr.press/v162/wang22al.html)

[Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension](https://arxiv.org/abs/1910.13461)

# Multimodal-Applications

## Understanding
  
  [Learning audio-visual speech representation by masked multimodal cluster prediction](https://arxiv.org/abs/2201.02184)
  
  [Self-supervised multimodal opinion summarization](https://arxiv.org/abs/2105.13135)
  
  [HuBERT: How much can a bad teacher benefit ASR pre-training?](https://ieeexplore.ieee.org/abstract/document/9414460/)
  
  [Layoutlmv2: Multi-modal pre-training for visually-rich document understanding](https://arxiv.org/abs/2012.14740)
  
  [Structext: Structured text understanding with multi-modal transformers](https://dl.acm.org/doi/abs/10.1145/3474085.3475345)
  
  [Icdar2019 competition on scanned receipt ocr and information extraction](https://ieeexplore.ieee.org/abstract/document/8977955/)
  
  [Funsd: A dataset for form understanding in noisy scanned documents](https://ieeexplore.ieee.org/abstract/document/8892998/)
  
  [Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding](http://openaccess.thecvf.com/content/CVPR2022/html/Gu_XYLayoutLM_Towards_Layout-Aware_Multimodal_Networks_for_Visually-Rich_Document_Understanding_CVPR_2022_paper.html)
  
  [Multistage fusion with forget gate for multimodal summarization in open-domain videos](https://aclanthology.org/2020.emnlp-main.144/)
  
  [Multimodal abstractive summarization for how2 videos](https://arxiv.org/abs/1906.07901)
  
  [Vision guided generative pre-trained language models for multimodal abstractive summarization](https://arxiv.org/abs/2109.02401)
  
  [How2: a large-scale dataset for multimodal language understanding](https://arxiv.org/abs/1811.00347)
  
  [wav2vec 2.0: A framework for self-supervised learning of speech representations](https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html)
  
  [Decoar 2.0: Deep contextualized acoustic representations with vector quantization](https://arxiv.org/abs/2012.06659)
  
  [LRS3-TED: a large-scale dataset for visual speech recognition](https://arxiv.org/abs/1809.00496)
  
  [Recurrent neural network transducer for audio-visual speech recognition](https://ieeexplore.ieee.org/abstract/document/9004036/)
  
  [Learning individual speaking styles for accurate lip to speech synthesis](http://openaccess.thecvf.com/content_CVPR_2020/html/Prajwal_Learning_Individual_Speaking_Styles_for_Accurate_Lip_to_Speech_Synthesis_CVPR_2020_paper.html)
  
  [On the importance of super-Gaussian speech priors for machine-learning based speech enhancement](https://ieeexplore.ieee.org/abstract/document/8121999/)
  
  [Active appearance models](https://link.springer.com/chapter/10.1007/BFb0054760)
  
  [Leveraging category information for single-frame visual sound source separation](https://ieeexplore.ieee.org/abstract/document/9484036/)
  
  [The sound of pixels](http://openaccess.thecvf.com/content_ECCV_2018/html/Hang_Zhao_The_Sound_of_ECCV_2018_paper.html)

## Classification

[Vqa: Visual question answering](http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)

[Topic-based content and sentiment analysis of Ebola virus on Twitter and in the news](https://journals.sagepub.com/doi/pdf/10.1177/0165551515608733)

[On the role of text preprocessing in neural network architectures: An evaluation study on text categorization and sentiment analysis](https://arxiv.org/abs/1707.01780)

[Market strategies used by processed food manufacturers to increase and consolidate their power: a systematic review and document analysis](https://globalizationandhealth.biomedcentral.com/articles/10.1186/s12992-021-00667-7)

[Swafn: Sentimental words aware fusion network for multimodal sentiment analysis](https://aclanthology.org/2020.coling-main.93/)

[Adaptive online event detection in news streams](https://www.sciencedirect.com/science/article/pii/S0950705117304550)

[Multi-source multimodal data and deep learning for disaster response: A systematic review](https://link.springer.com/article/10.1007/s42979-021-00971-4)

[A survey of data representation for multi-modality event detection and evolution](https://www.mdpi.com/2076-3417/12/4/2204)

[Crisismmd: Multimodal twitter datasets from natural disasters](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17816)

[Multi-modal generative adversarial networks for traffic event detection in smart cities](https://www.sciencedirect.com/science/article/pii/S0957417421003808)

[Proppy: Organizing the news based on their propagandistic content](https://www.sciencedirect.com/science/article/pii/S0306457318306058)

[Fine-grained analysis of propaganda in news article](https://aclanthology.org/D19-1565/)

[Multimodal fusion with recurrent neural networks for rumor detection on microblogs](https://dl.acm.org/doi/abs/10.1145/3123266.3123454)

[SAFE: Similarity-Aware Multi-modal Fake News Detection](https://link.springer.com/chapter/10.1007/978-3-030-47436-2_27)

[From recognition to cognition: Visual commonsense reasoning](http://openaccess.thecvf.com/content_CVPR_2019/html/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.html)

[Kvl-bert: Knowledge enhanced visual-and-linguistic bert for visual commonsense reasoning](https://www.sciencedirect.com/science/article/pii/S0950705121006705)

[Lxmert: Learning cross-modality encoder representations from transformers](https://arxiv.org/abs/1908.07490)

[Pixel-bert: Aligning image pixels with text by deep multi-modal transformers](https://arxiv.org/abs/2004.00849)

[Vision-language navigation with self-supervised auxiliary reasoning tasks](http://openaccess.thecvf.com/content_CVPR_2020/html/Zhu_Vision-Language_Navigation_With_Self-Supervised_Auxiliary_Reasoning_Tasks_CVPR_2020_paper.html)

## Generation

[Recent advances and trends in multimodal deep learning: A review](https://arxiv.org/abs/2105.11087)

[Vqa: Visual question answering](http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)

[Microsoft COCO: common objects in context](https://arxiv.org/pdf/1405.0312.pdf%090.949.pdf)

[Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)

[Distributed representations of words and phrases and their compositionality](https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)

[LRS3-TED: a large-scale dataset for visual speech recognition](https://arxiv.org/abs/1809.00496)

[A lip sync expert is all you need for speech to lip generation in the wild](https://dl.acm.org/doi/abs/10.1145/3394171.3413532)

[Unified vision-language pre-training for image captioning and vqa](https://ojs.aaai.org/index.php/AAAI/article/view/7005)

[Show and tell: A neural image caption generator](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html)

[Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning](http://openaccess.thecvf.com/content_cvpr_2017/html/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.html)

[Self-critical sequence training for image captioning](http://openaccess.thecvf.com/content_cvpr_2017/html/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.html)

[Visual question answering: A survey of methods and datasets](https://www.sciencedirect.com/science/article/pii/S1077314217300772)

[How to find a good image-text embedding for remote sensing visual question answering?](https://arxiv.org/abs/2109.11848)

[An improved attention for visual question answering](https://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Rahman_An_Improved_Attention_for_Visual_Question_Answering_CVPRW_2021_paper.html)

[Analyzing Compositionality in Visual Question Answering.](https://vigilworkshop.github.io/static/papers-2019/43.pdf)

[Ok-vqa: A visual question answering benchmark requiring external knowledge](http://openaccess.thecvf.com/content_CVPR_2019/html/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.html)

[Multibench: Multiscale benchmarks for multimodal representation learning](https://arxiv.org/abs/2107.07502)

[Benchmarking multimodal automl for tabular data with text fields](https://arxiv.org/abs/2111.02705)

[Multimodal explanations: Justifying decisions and pointing to the evidence](http://openaccess.thecvf.com/content_cvpr_2018/html/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.html)

[Don't just assume; look and answer: Overcoming priors for visual question answering](http://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html)

[Generative adversarial text to image synthesis](http://proceedings.mlr.press/v48/reed16.html)

[The caltech-ucsd birds-200-2011 dataset](https://authors.library.caltech.edu/27452/)

[Attngan: Fine-grained text to image generation with attentional generative adversarial networks](http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html)

[LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading.](https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/1393.pdf)

[The conversation: Deep audio-visual speech enhancement](https://arxiv.org/abs/1804.04121)

[TCD-TIMIT: An audio-visual corpus of continuous speech](https://ieeexplore.ieee.org/abstract/document/7050271/)

[Deep voice 3: Scaling text-to-speech with convolutional sequence learning](https://arxiv.org/abs/1710.07654)

[Natural tts synthesis by conditioning wavenet on mel spectrogram predictions](https://ieeexplore.ieee.org/abstract/document/8461368/)

[Vid2speech: speech reconstruction from silent video](https://ieeexplore.ieee.org/abstract/document/7953127/)

[Lip2audspec: Speech reconstruction from silent lip movements video](https://ieeexplore.ieee.org/abstract/document/8461856/)

[Video-driven speech reconstruction using generative adversarial networks](https://arxiv.org/abs/1906.06301)

## Retrieval

[Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html)

[Learning robust patient representations from multi-modal electronic health records: a supervised deep learning approach](https://epubs.siam.org/doi/abs/10.1137/1.9781611976700.66)

[Referring expression comprehension: A survey of methods and datasets](https://ieeexplore.ieee.org/abstract/document/9285213/)

[Vl-bert: Pre-training of generic visual-linguistic representations](https://arxiv.org/abs/1908.08530)

[Clinically accurate chest x-ray report generation](http://proceedings.mlr.press/v106/liu19a.html)

## Translation

[Deep residual learning for image recognition](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)

[Probing the need for visual context in multimodal machine translation](https://arxiv.org/abs/1903.08678)

[Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)

[Multi-modal neural machine translation with deep semantic interactions](https://www.sciencedirect.com/science/article/pii/S0020025520311105)

# Multimodal-Datasets

[Vqa: Visual question answering](http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)

[Microsoft COCO: common objects in context](https://arxiv.org/pdf/1405.0312.pdf%090.949.pdf)

[Pre-training technique to localize medical bert and enhance biomedical bert](https://arxiv.org/abs/2005.07202)

[Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models](http://openaccess.thecvf.com/content_iccv_2015/html/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.html)

[Icdar2019 competition on scanned receipt ocr and information extraction](https://ieeexplore.ieee.org/abstract/document/8977955/)

[Funsd: A dataset for form understanding in noisy scanned documents](https://ieeexplore.ieee.org/abstract/document/8892998/)

[How2: a large-scale dataset for multimodal language understanding](https://arxiv.org/abs/1811.00347)

[Learning individual speaking styles for accurate lip to speech synthesis](http://openaccess.thecvf.com/content_CVPR_2020/html/Prajwal_Learning_Individual_Speaking_Styles_for_Accurate_Lip_to_Speech_Synthesis_CVPR_2020_paper.html)

[The sound of pixels](http://openaccess.thecvf.com/content_ECCV_2018/html/Hang_Zhao_The_Sound_of_ECCV_2018_paper.html)

[Crisismmd: Multimodal twitter datasets from natural disasters](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17816)

[From recognition to cognition: Visual commonsense reasoning](http://openaccess.thecvf.com/content_CVPR_2019/html/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.html)

[The caltech-ucsd birds-200-2011 dataset](https://authors.library.caltech.edu/27452/)

[Framing image description as a ranking task: Data, models and evaluation metrics](https://www.jair.org/index.php/jair/article/view/10833)

[Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph](https://aclanthology.org/P18-1208/)

[MIMIC-III, a freely accessible critical care database](https://www.nature.com/articles/sdata201635)

[Fashion 200K Benchmark](https://github.com/xthan/fashion-200k)

[Indoor scene segmentation using a structured light sensor](https://ieeexplore.ieee.org/abstract/document/6130298/)

[Indoor segmentation and support inference from rgbd images.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/shkf_eccv2012.pdf)

[Good news, everyone! context driven entity-aware captioning for news images](http://openaccess.thecvf.com/content_CVPR_2019/html/Biten_Good_News_Everyone_Context_Driven_Entity-Aware_Captioning_for_News_Images_CVPR_2019_paper.html)

[Msr-vtt: A large video description dataset for bridging video and language](http://openaccess.thecvf.com/content_cvpr_2016/html/Xu_MSR-VTT_A_Large_CVPR_2016_paper.html)

[Video question answering via gradually refined attention over appearance and motion](https://dl.acm.org/doi/abs/10.1145/3123266.3123427)

[Tgif-qa: Toward spatio-temporal reasoning in visual question answering](http://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html)

[Multi-target embodied question answering](http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.html)

[Videonavqa: Bridging the gap between visual and embodied question answering](https://arxiv.org/abs/1908.04950)

[An analysis of visual question answering algorithms](http://openaccess.thecvf.com/content_iccv_2017/html/Kafle_An_Analysis_of_ICCV_2017_paper.html)

[nuscenes: A multimodal dataset for autonomous driving](http://openaccess.thecvf.com/content_CVPR_2020/html/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.html)

[Automated flower classification over a large number of classes](https://ieeexplore.ieee.org/abstract/document/4756141/)

[Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos](https://arxiv.org/abs/1606.06259)

[Can we read speech beyond the lips? rethinking roi selection for deep visual speech recognition](https://ieeexplore.ieee.org/abstract/document/9320240/)

[The mit stata center dataset](https://journals.sagepub.com/doi/pdf/10.1177/0278364913509035)

[Data2vec: A general framework for self-supervised learning in speech, vision and language](https://proceedings.mlr.press/v162/baevski22a.html)

[Flava: A foundational language and vision alignment model](http://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html)

[Uc2: Universal cross-lingual cross-modal vision-and-language pre-training](http://openaccess.thecvf.com/content/CVPR2021/html/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.html)

# Citation
If you find the listing and survey useful for your work, please cite the paper:
```
@article{manzoor2023multimodality,
  title={Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications},
  author={Manzoor, Muhammad Arslan and Albarri, Sarah and Xian, Ziting and Meng, Zaiqiao and Nakov, Preslav and Liang, Shangsong},
  journal={arXiv preprint arXiv:2302.00389},
  year={2023}
}
```
