

@article{co-learn,
  author    = {Anil Rahate and
               Rahee Walambe and
               Sheela Ramanna and
               Ketan Kotecha},
  title     = {Multimodal Co-learning: Challenges, Applications with Datasets, Recent
               Advances and Future Directions},
  journal   = {CoRR},
  volume    = {abs/2107.13782},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.13782},
  eprinttype = {arXiv},
  eprint    = {2107.13782},
  timestamp = {Tue, 03 Aug 2021 14:53:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-13782.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fusion,
    author = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
    title = "{A Survey on Deep Learning for Multimodal Data Fusion}",
    journal = {Neural Computation},
    volume = {32},
    number = {5},
    pages = {829-864},
    year = {2020},
    month = {05},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01273},
    url = {https://doi.org/10.1162/neco\_a\_01273},
    eprint = {https://direct.mit.edu/neco/article-pdf/32/5/829/1865303/neco\_a\_01273.pdf},
}


@article{translation,
  author    = {Umut Sulubacak and
               Ozan Caglayan and
               Stig{-}Arne Gr{\"{o}}nroos and
               Aku Rouhe and
               Desmond Elliott and
               Lucia Specia and
               J{\"{o}}rg Tiedemann},
  title     = {Multimodal Machine Translation through Visuals and Speech},
  journal   = {CoRR},
  volume    = {abs/1911.12798},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.12798},
  eprinttype = {arXiv},
  eprint    = {1911.12798},
  timestamp = {Thu, 14 Oct 2021 09:15:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-12798.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{im2021self,
  title={Self-supervised multimodal opinion summarization},
  author={Im, Jinbae and Kim, Moonki and Lee, Hoyeop and Cho, Hyunsouk and Chung, Sehee},
  journal={arXiv preprint arXiv:2105.13135},
  year={2021}
}
@article{xu2020layoutlmv2,
  title={Layoutlmv2: Multi-modal pre-training for visually-rich document understanding},
  author={Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and others},
  journal={arXiv preprint arXiv:2012.14740},
  year={2020}
}
@inproceedings{li2021structext,
  title={StrucTexT: Structured Text Understanding with Multi-Modal Transformers},
  author={Li, Yulin and Qian, Yuxi and Yu, Yuechen and Qin, Xiameng and Zhang, Chengquan and Liu, Yan and Yao, Kun and Han, Junyu and Liu, Jingtuo and Ding, Errui},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={1912--1920},
  year={2021}
}

@article{camacho2017role,
  title={On the role of text preprocessing in neural network architectures: An evaluation study on text categorization and sentiment analysis},
  author={Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
  journal={arXiv preprint arXiv:1707.01780},
  year={2017}
}
@article{zhou2020safe,
  title={Safe: similarity-aware multi-modal fake news detection (2020)},
  author={Zhou, Xinyi and Wu, Jindi and Zafarani, Reza},
  journal={Preprint. arXiv},
  volume={200304981},
  year={2020}
}
@inproceedings{huang2019icdar2019,
  title={Icdar2019 competition on scanned receipt ocr and information extraction},
  author={Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1516--1520},
  year={2019},
  organization={IEEE}
}
@inproceedings{alam2018crisismmd,
  title={Crisismmd: Multimodal twitter datasets from natural disasters},
  author={Alam, Firoj and Ofli, Ferda and Imran, Muhammad},
  booktitle={Twelfth international AAAI conference on web and social media},
  year={2018}
}
@article{chen2021multi,
  title={Multi-modal generative adversarial networks for traffic event detection in smart cities},
  author={Chen, Qi and Wang, Wei and Huang, Kaizhu and De, Suparna and Coenen, Frans},
  journal={Expert Systems with Applications},
  volume={177},
  pages={114939},
  year={2021},
  publisher={Elsevier}
}
@article{xiao2022survey,
  title={A Survey of Data Representation for Multi-Modality Event Detection and Evolution},
  author={Xiao, Kejing and Qian, Zhaopeng and Qin, Biao},
  journal={Applied Sciences},
  volume={12},
  number={4},
  pages={2204},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{algiriyage2022multi,
  title={Multi-source Multimodal Data and Deep Learning for Disaster Response: A Systematic Review},
  author={Algiriyage, Nilani and Prasanna, Raj and Stock, Kristin and Doyle, Emma EH and Johnston, David},
  journal={SN Computer Science},
  volume={3},
  number={1},
  pages={1--29},
  year={2022},
  publisher={Springer}
}
@article{hu2017adaptive,
  title={Adaptive online event detection in news streams},
  author={Hu, Linmei and Zhang, Bin and Hou, Lei and Li, Juanzi},
  journal={Knowledge-Based Systems},
  volume={138},
  pages={105--112},
  year={2017},
  publisher={Elsevier}
}
@inproceedings{jaume2019funsd,
  title={Funsd: A dataset for form understanding in noisy scanned documents},
  author={Jaume, Guillaume and Ekenel, Hazim Kemal and Thiran, Jean-Philippe},
  booktitle={2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)},
  volume={2},
  pages={1--6},
  year={2019},
  organization={IEEE}
}
@article{wood2021market,
  title={Market strategies used by processed food manufacturers to increase and consolidate their power: a systematic review and document analysis},
  author={Wood, Benjamin and Williams, Owain and Nagarajan, Vijaya and Sacks, Gary},
  journal={Globalization and health},
  volume={17},
  number={1},
  pages={1--23},
  year={2021},
  publisher={BioMed Central}
}
@inproceedings{chen-li-2020-swafn,
    title = "{SWAFN}: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis",
    author = "Chen, Minping  and
      Li, Xia",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.93",
    doi = "10.18653/v1/2020.coling-main.93",
    pages = "1067--1077",
    abstract = "Multimodal sentiment analysis aims to predict sentiment of language text with the help of other modalities, such as vision and acoustic features. Previous studies focused on learning the joint representation of multiple modalities, ignoring some useful knowledge contained in language modal. In this paper, we try to incorporate sentimental words knowledge into the fusion network to guide the learning of joint representation of multimodal features. Our method consists of two components: shallow fusion part and aggregation part. For the shallow fusion part, we use crossmodal coattention mechanism to obtain bidirectional context information of each two modals to get the fused shallow representations. For the aggregation part, we design a multitask of sentimental words classification to help and guide the deep fusion of the three modalities and obtain the final sentimental words aware fusion representation. We carry out several experiments on CMU-MOSI, CMU-MOSEI and YouTube datasets. The experimental results show that introducing sentimental words prediction as a multitask can really improve the fusion representation of multiple modalities.",
}

@article{kim2016topic,
  title={Topic-based content and sentiment analysis of Ebola virus on Twitter and in the news},
  author={Kim, Erin Hea-Jin and Jeong, Yoo Kyung and Kim, Yuyoung and Kang, Keun Young and Song, Min},
  journal={Journal of Information Science},
  volume={42},
  number={6},
  pages={763--781},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{ramachandram2017deep,
  title={Deep multimodal learning: A survey on recent advances and trends},
  author={Ramachandram, Dhanesh and Taylor, Graham W},
  journal={IEEE signal processing magazine},
  volume={34},
  number={6},
  pages={96--108},
  year={2017},
  publisher={IEEE}
}
@article{guo2019deep,
  title={Deep multimodal representation learning: A survey},
  author={Guo, Wenzhong and Wang, Jianwen and Wang, Shiping},
  journal={IEEE Access},
  volume={7},
  pages={63373--63394},
  year={2019},
  publisher={IEEE}
}
@article{stappen2021multimodal,
  title={The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements},
  author={Stappen, Lukas and Baird, Alice and Schumann, Lea and Bjorn, Schuller},
  journal={IEEE Transactions on Affective Computing},
  year={2021},
  publisher={IEEE}
}
@article{bayoudh2021survey,
  title={A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets},
  author={Bayoudh, Khaled and Knani, Raja and Hamdaoui, Fay{\c{c}}al and Mtibaa, Abdellatif},
  journal={The Visual Computer},
  pages={1--32},
  year={2021},
  publisher={Springer}
}
@article{chandrasekaran2021multimodal,
  title={Multimodal sentimental analysis for social media applications: A comprehensive review},
  author={Chandrasekaran, Ganesh and Nguyen, Tu N and Hemanth D, Jude},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={11},
  number={5},
  pages={e1415},
  year={2021},
  publisher={Wiley Online Library}
}
@article{baltruvsaitis2018multimodal,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={2},
  pages={423--443},
  year={2018},
  publisher={IEEE}
}
@article{mcgurk1976hearing,
  title={Hearing lips and seeing voices},
  author={McGurk, Harry and MacDonald, John},
  journal={Nature},
  volume={264},
  number={5588},
  pages={746--748},
  year={1976},
  publisher={Nature Publishing Group}
}
@article{wang2022fmfn,
  title={FMFN: Fine-Grained Multimodal Fusion Networks for Fake News Detection},
  author={Wang, Jingzi and Mao, Hongyan and Li, Hongwei},
  journal={Applied Sciences},
  volume={12},
  number={3},
  pages={1093},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{wignell2021natural,
  title={Natural language understanding and multimodal discourse analysis for interpreting extremist communications and the re-use of these materials online},
  author={Wignell, Peter and Chai, Kevin and Tan, Sabine and Oâ€™Halloran, Kay and Lange, Rebecca},
  journal={Terrorism and political violence},
  volume={33},
  number={1},
  pages={71--95},
  year={2021},
  publisher={Taylor \& Francis}
}
@inproceedings{jin2017multimodal,
  title={Multimodal fusion with recurrent neural networks for rumor detection on microblogs},
  author={Jin, Zhiwei and Cao, Juan and Guo, Han and Zhang, Yongdong and Luo, Jiebo},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={795--816},
  year={2017}
}
@inproceedings{zhang2021learning,
  title={Learning robust patient representations from multi-modal electronic health records: a supervised deep learning approach},
  author={Zhang, Xianli and Qian, Buyue and Li, Yang and Liu, Yang and Chen, Xi and Guan, Chong and Li, Chen},
  booktitle={Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)},
  pages={585--593},
  year={2021},
  organization={SIAM}
}
@article{habibian2016video2vec,
  title={Video2vec embeddings recognize events when examples are scarce},
  author={Habibian, Amirhossein and Mensink, Thomas and Snoek, Cees GM},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={10},
  pages={2089--2103},
  year={2016},
  publisher={IEEE}
}
@inproceedings{rasiwasia2010new,
  title={A new approach to cross-modal multimedia retrieval},
  author={Rasiwasia, Nikhil and Costa Pereira, Jose and Coviello, Emanuele and Doyle, Gabriel and Lanckriet, Gert RG and Levy, Roger and Vasconcelos, Nuno},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={251--260},
  year={2010}
}
@article{lecun2015deep,
  title={Deep learning. nature, 521 (7553), 436-444},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey and others},
  journal={Google Scholar Google Scholar Cross Ref Cross Ref},
  year={2015}
}
@inproceedings{chen2020swafn,
  title={SWAFN: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis},
  author={Chen, Minping and Li, Xia},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={1067--1077},
  year={2020}
}


@article{vqa_att,
  author    = {Tanzila Rahman and
               Shih{-}Han Chou and
               Leonid Sigal and
               Giuseppe Carenini},
  title     = {An Improved Attention for Visual Question Answering},
  journal   = {CoRR},
  volume    = {abs/2011.02164},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.02164},
  eprinttype = {arXiv},
  eprint    = {2011.02164},
  timestamp = {Fri, 06 Nov 2020 15:32:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-02164.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{vqa_comp,
  author={Sanjay Subramanian and Sameer Singh and Matt Gardner},
  title={Analyzing Compositionality in Visual Question Answering},
  year={2019},
  booktitle={ViGIL@NeurIPS},
}
@article{vqa_kb,
  author    = {Kenneth Marino and
               Mohammad Rastegari and
               Ali Farhadi and
               Roozbeh Mottaghi},
  title     = {{OK-VQA:} {A} Visual Question Answering Benchmark Requiring External
               Knowledge},
  journal   = {CoRR},
  volume    = {abs/1906.00067},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.00067},
  eprinttype = {arXiv},
  eprint    = {1906.00067},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-00067.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{vqa_emb,
  author    = {Christel Chappuis and
               Sylvain Lobry and
               Benjamin Kellenberger and
               Bertrand Le Saux and
               Devis Tuia},
  title     = {How to find a good image-text embedding for remote sensing visual
               question answering?},
  journal   = {CoRR},
  volume    = {abs/2109.11848},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.11848},
  eprinttype = {arXiv},
  eprint    = {2109.11848},
  timestamp = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-11848.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{bench2,
  author    = {Xingjian Shi and
               Jonas Mueller and
               Nick Erickson and
               Mu Li and
               Alexander J. Smola},
  title     = {Benchmarking Multimodal AutoML for Tabular Data with Text Fields},
  journal   = {CoRR},
  volume    = {abs/2111.02705},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.02705},
  eprinttype = {arXiv},
  eprint    = {2111.02705},
  timestamp = {Fri, 05 Nov 2021 15:25:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-02705.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{VQA,
  author    = {Qi Wu and
               Damien Teney and
               Peng Wang and
               Chunhua Shen and
               Anthony R. Dick and
               Anton van den Hengel},
  title     = {Visual Question Answering: {A} Survey of Methods and Datasets},
  journal   = {CoRR},
  volume    = {abs/1607.05910},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.05910},
  eprinttype = {arXiv},
  eprint    = {1607.05910},
  timestamp = {Tue, 19 Mar 2019 13:03:53 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/WuTWSDH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{VQA2,
  author    = {Stanislaw Antol and
               Aishwarya Agrawal and
               Jiasen Lu and
               Margaret Mitchell and
               Dhruv Batra and
               C. Lawrence Zitnick and
               Devi Parikh},
  title     = {{VQA:} Visual Question Answering},
  journal   = {CoRR},
  volume    = {abs/1505.00468},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.00468},
  eprinttype = {arXiv},
  eprint    = {1505.00468},
  timestamp = {Mon, 13 Aug 2018 16:48:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AntolALMBZP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vqa-cp,
  author    = {Aishwarya Agrawal and
               Dhruv Batra and
               Devi Parikh and
               Aniruddha Kembhavi},
  title     = {Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question
               Answering},
  journal   = {CoRR},
  volume    = {abs/1712.00377},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.00377},
  eprinttype = {arXiv},
  eprint    = {1712.00377},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-00377.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vqa-x,
  author    = {Dong Huk Park and
               Lisa Anne Hendricks and
               Zeynep Akata and
               Bernt Schiele and
               Trevor Darrell and
               Marcus Rohrbach},
  title     = {Attentive Explanations: Justifying Decisions and Pointing to the Evidence},
  journal   = {CoRR},
  volume    = {abs/1612.04757},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.04757},
  eprinttype = {arXiv},
  eprint    = {1612.04757},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ParkHASDR16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{recent_paper,
  author    = {Jabeen Summaira and
               Xi Li and
               Amin Muhammad Shoib and
               Songyuan Li and
               Jabbar Abdul},
  title     = {Recent Advances and Trends in Multimodal Deep Learning: {A} Review},
  journal   = {CoRR},
  volume    = {abs/2105.11087},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.11087},
  eprinttype = {arXiv},
  eprint    = {2105.11087},
  timestamp = {Tue, 01 Jun 2021 18:07:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-11087.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bench1,
  author    = {Paul Pu Liang and
               Yiwei Lyu and
               Xiang Fan and
               Zetian Wu and
               Yun Cheng and
               Jason Wu and
               Leslie Chen and
               Peter Wu and
               Michelle A. Lee and
               Yuke Zhu and
               Ruslan Salakhutdinov and
               Louis{-}Philippe Morency},
  title     = {MultiBench: Multiscale Benchmarks for Multimodal Representation Learning},
  journal   = {CoRR},
  volume    = {abs/2107.07502},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.07502},
  eprinttype = {arXiv},
  eprint    = {2107.07502},
  timestamp = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-07502.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{EHR,
  author    = {Wei{-}Hung Weng and
               Peter Szolovits},
  title     = {Representation Learning for Electronic Health Records},
  journal   = {CoRR},
  volume    = {abs/1909.09248},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.09248},
  eprinttype = {arXiv},
  eprint    = {1909.09248},
  timestamp = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09248.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}








